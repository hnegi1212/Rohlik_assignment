# Rohlik Group — ML Engineer Assignment (Forecasting + Optimal Pricing)

This README explains **what the script does, why each approach was chosen, and how results are validated**.  
The goal is to make the code easy to review and to show the reasoning behind model/optimizer choices.

---

## 1) Problem Statement

### Objective 2 — Sales Forecasting
Predict sales for each of 5 products for the next **7 days** assuming:
- `sell_price` stays the same as the **last historical day**

### Objective 1 — Optimal Pricing
Choose `sell_price` for each product for each of the next **7 days** to maximize:
- **Total 7-day revenue**

Subject to a hard constraint:
- **Total 7-day profit ≥ 7 × profit from the last historical day**

Profit and revenue:
- Revenue = `sales * sell_price`
- Profit = `(sell_price - buy_price) * sales`

Important requirement:
- Use **last day’s buy_price** for all future 7 days.

---

## 2) Inputs and Outputs

- `Rohlik_assignment.py` — main end-to-end solution script (forecasting + demand modeling + pricing optimization)
- `Data_analysis.ipynb` — notebook for **EDA and visualization** (raw data + forecasts + optimized pricing)

### Inputs
- `ml_task_data.csv`

Key columns:
- `product_id`, `date`, `sales`, `sell_price`, `margin`

Derived:
- `buy_price = sell_price - margin`

### Outputs (generated by the script)
- `objective2_forecast.csv`  
  7-day sales forecast (fixed price) with expected revenue/profit
- `objective1_optimal_pricing.csv`  
  7-day optimized pricing plan with predicted sales/revenue/profit
- `model_selection.csv`  
  Which model was selected per product (forecasting + demand)
- `approach_comparison.csv`  
  Comparison of the 3 pricing optimizers (revenue, profit, feasibility)

---

## 3) Design Philosophy (Why This Approach)

This is intentionally written like a production take-home:
- **Baselines first** → prove that ML adds value
- **Time-based validation** → avoid leakage, mimic deployment
- **Global models for forecasting** → stronger generalization for products with limited history
- **Monotonic constraints for demand modeling** → prevent unsafe optimization artifacts
- **Multiple optimizers** → show both pragmatic and theoretically principled solutions
- **Traceability outputs** (`model_selection.csv`, `approach_comparison.csv`) → reviewer-friendly

---

## 4) Feature Engineering (Why These Features)

### Calendar Features (`add_calendar_features`)
Adds:
- `dow` (day-of-week), `is_weekend`
- yearly seasonality via `sin_doy`, `cos_doy`
- trend index `t`

**Why**
Retail and grocery demand is typically driven by:
- weekly patterns (weekday/weekend)
- seasonal variation
- slow-moving trend (growth/decline)

These are stable, explainable, and work well out-of-sample.

### Lag & Rolling Features (`add_lag_rolling_features`)
Adds:
- Lags: `lag_1`, `lag_7`
- Rolling windows: mean/std for 7 and 14 days (`roll_mean_7`, `roll_std_7`, ...)

**Why**
Short-horizon forecasting is dominated by:
- recency (yesterday)
- weekly autocorrelation (same weekday last week)
- momentum/volatility patterns (rolling stats)

**Leakage prevention**
All rolling features are computed on `shift(1)` so that:
- today’s features never use today’s sales target.

---

## 5) Metrics Used (What and Why)

### sMAPE (Symmetric Mean Absolute Percentage Error)
Implemented as `smape(y_true, y_pred)`.

**Why sMAPE**
- Scale-independent: can compare products with different sales volumes
- Robust when absolute sales differ significantly by product
- Common in forecasting tasks

Formula:
\[
sMAPE = mean\left(\frac{|y-\hat{y}|}{(|y|+|\hat{y}|)/2}\right)
\]

Note: For practical stability the code uses a small epsilon in the denominator.

---

## 6) Objective 2 — Forecasting (3 approaches, selection per product)

### Why three approaches?
Because in retail:
- Simple seasonality baselines can be strong.
- ML should be used only if it improves accuracy measurably.
- Using multiple approaches demonstrates good ML practice and avoids “model worship”.

### Approach A — Weekday Mean Baseline
Predict:
- sales = historical mean sales for that product and that weekday

**Why**
- Captures weekly seasonality with zero complexity
- Strong baseline in grocery demand
- Provides a “minimum bar” that ML must beat

### Approach B — CatBoost Global Model
Model:
- CatBoostRegressor trained on all products (global model)
- `product_id` treated as a categorical feature

Target transform:
- Train on `log1p(sales)`, then invert via `expm1`

**Why CatBoost**
- Strong on tabular data with categorical features
- Nonlinear modeling capability (better than linear)
- Often used in real retail settings

**Why global**
- Shares information across products (common seasonal patterns)
- Helps products with less stable history

### Approach C — LightGBM Quantile Model (Median)
Model:
- LightGBM with `objective="quantile"`, `alpha=0.5` (median)

**Why quantile**
- Median forecasts are more robust to spikes/outliers
- Signals awareness of probabilistic forecasting thinking
- Can later extend to risk-aware pricing (e.g., lower-quantile demand)

### Model selection (per product)
The script evaluates A/B/C on a time-holdout:
- last 30 days per product → validation
- choose lowest sMAPE per product

Outputs:
- `model_selection.csv` includes sMAPE values and chosen forecaster.

### Forecast generation
For ML models, forecasting is **recursive**:
- day 1 uses real lags
- day 2 uses predicted day 1 sales as lag input
- continues up to day 7

**Why recursion**
Lag features require future sales values that are unknown—recursion is the standard production technique.

---

## 7) Objective 1 — Demand Modeling for Pricing (3 approaches)

Pricing requires **counterfactual prediction**:
- sales as a function of price, not just time.

This is harder than forecasting because:
- the model is used inside an optimizer
- unrealistic demand curves can cause unsafe price outputs.

### Why three demand models?
To balance:
- interpretability (for business)
- performance (for accuracy)
- safety (for optimization)

### Demand Model 1 — Log–Log Ridge (Elasticity-style)
Model:
- `log1p(sales) ~ log(price) + calendar`

**Why**
- Elasticity interpretation is common in pricing
- Ridge makes it stable with limited/noisy data
- Strong, explainable baseline

### Demand Model 2 — LightGBM with Monotonic Constraint
Model:
- LightGBM regressor with `monotone_constraints=[-1,0,0,0,0,0]`

Meaning:
- predicted demand must be **monotonically decreasing** with respect to `sell_price`

**Why monotonic**
Unconstrained ML can learn artifacts like:
- “higher price increases demand”
The optimizer would exploit this and pick extreme prices.

Monotonic constraints enforce economically consistent behavior and produce safer optimization.

### Demand Model 3 — CatBoost with Monotonic Constraint
Same monotonic idea, different model family.

**Why include**
- provides an alternative bias/variance tradeoff
- sometimes wins vs LightGBM depending on data

### Demand model selection (per product)
- time holdout: last max(7, 20% of data) up to 30 days
- choose lowest sMAPE
- refit on full data for final usage

---

## 8) Candidate Price Grid (Why Discrete Prices)

Pricing optimization is done over **candidate prices** rather than continuous.

`candidate_prices()`:
- Starts from historical observed prices
- Removes prices ≤ buy_price (avoid negative unit profit)
- Adds small uplift range up to `PRICE_UPLIFT_CAP` (default +20%)
- Caps total candidates to `MAX_PRICE_CANDIDATES` via quantiles

**Why**
- Real retail pricing often occurs on discrete levels
- Keeps optimization explainable
- Prevents unrealistic price jumps
- Makes optimization fast and deterministic

---

## 9) Objective 1 — Pricing Optimization (3 approaches)

We optimize:
- maximize total expected revenue over 7 days
- subject to total expected profit ≥ threshold

### Approach A1 — Constant Price per Product (baseline)
Pick **one price per product** for all 7 days.

**Why**
- Highly interpretable and operationally simple
- Useful baseline and fallback
- Shows business awareness (stable pricing)

### Approach A2 — Greedy Profit Repair (practical workhorse)
Steps:
1) Start with revenue-max solution (λ=0)
2) If profit constraint not met:
   - increase prices only where it improves profit efficiently
3) stop when constraint satisfied

**Why**
- Very fast and robust
- Often near-optimal under tight constraints
- Pragmatic “production style” solver

This is why it often wins in practice and did so in your run.

### Approach A3 — Lagrangian Relaxation (principled constrained method)
Solve:
- maximize `revenue + λ * profit`

Then:
- binary search λ until profit constraint is met

**Why**
- Standard constrained optimization technique
- Works well for discrete action spaces
- Good theoretical grounding and often yields strong solutions

### Final selection
Among all feasible solutions:
- choose highest total expected revenue

This ensures the output is the best feasible plan among the compared approaches.

---

## 10) How Profit Constraint is Computed

1) Compute last-day profit from raw data:
- per product profit(last day) = `margin_last_day * sales_last_day`
- sum across products → total last-day profit

2) Required profit:
- `profit_min_total = 7 * profit_last_day_total`

3) For each candidate future plan:
- use **last day buy_price** per product (fixed)
- compute `expected_profit = (sell_price - buy_last) * predicted_sales`
- sum across all products/days
- must satisfy threshold

---

## 11) How to Run

Install dependencies:
```bash
pip install -U pandas numpy scikit-learn lightgbm catboost

Run: python Rohlik_assignment.py

